<article class="devsite-article">
 <article class="devsite-article-inner">
  <h1 class="devsite-page-title">
   Release Notes
  </h1>
  <devsite-toc class="devsite-nav" devsite-toc-embedded="">
  </devsite-toc>
  <div class="devsite-article-body clearfix">
   <section class="intro">
    <p>
     This page documents production updates to Anthos GKE on-prem. You can
    periodically check this page for announcements about new or updated features,
    bug fixes, known issues, and deprecated functionality.
    </p>
    <p>
     See also:
     <ul>
      <li>
       <a href="/anthos/gke/docs/on-prem/downloads">
        Downloads
       </a>
      </li>
      <li>
       <a href="/anthos/gke/docs/on-prem/versioning-and-upgrades">
        Versioning and upgrades
       </a>
      </li>
      <li>
       <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
        Upgrading GKE on-prem
       </a>
      </li>
     </ul>
    </p>
   </section>
   <section class="xml">
    <p>
     To get the latest product updates delivered to you, add the URL of this page to your
     <a class="external" href="https://wikipedia.org/wiki/Comparison_of_feed_aggregators">
      feed
          reader
     </a>
     , or add the feed URL directly:
     <code dir="ltr" translate="no">
      https://cloud.google.com/feeds/gkeonprem-release-notes.xml
     </code>
    </p>
   </section>
   <section class="releases">
    <h2 id="november_19_2019">
     November 19, 2019
    </h2>
    <p>
     GKE on-prem version 1.1.2-gke.0 is now available. To download
  version 1.1.2-gke.0's OVA,
     <code dir="ltr" translate="no">
      gkectl
     </code>
     , and upgrade bundle, see
     <a href="/anthos/gke/docs/on-prem/downloads#latest">
      Downloads
     </a>
     . Then, see
     <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
      Upgrading admin workstation
     </a>
     and
     <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
      Upgrading clusters
     </a>
     .
    </p>
    <p>
     This patch version includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Published
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/security/hardening-your-cluster">
       Hardening your cluster
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Published
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/managing-clusters">
       Managing clusters
      </a>
      .
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed the known issue from
      <a href="#vsan-datadisk-issue">
       November 5
      </a>
      .
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed the known issue from
      <a href="#docker-registry-issue">
       November 8
      </a>
      .
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      If you are running multiple data centers in vSphere, running
      <code dir="ltr" translate="no">
       gkectl diagnose cluster
      </code>
      might return the following error,
    which you can safely ignore:
     </p>
     <pre dir="ltr" translate="no">Checking storage...FAIL
path '*' resolves to multiple datacenters</pre>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      If you are running a vSAN datastore, running
      <code dir="ltr" translate="no">
       gkectl diagnose cluster
      </code>
      might return the following error,
    which you can safely ignore:
     </p>
     <pre dir="ltr" translate="no">PersistentVolume [NAME]: virtual disk "[[DATASTORE_NAME]]
[PVC]" IS NOT attached to machine "[MACHINE_NAME]" but IS listed in the Node.Status</pre>
    </div>
    <h2 id="november_8_2019">
     November 8, 2019
    </h2>
    <div class="release-issue" id="docker-registry-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      In GKE on-prem version 1.1.1-gke.2, a known issue prevents
creation of clusters configured to use a Docker registry. You configure a
Docker registry by populating the GKE on-prem configuration file's
      <code dir="ltr" translate="no">
       privateregistryconfig
      </code>
      field. Cluster creation fails with an error
such as
      <code dir="ltr" translate="no">
       Failed to create root cluster: could not create external client:
could not create external control plane: docker run error: exit status 125
      </code>
      <p>
       A fix is targeted for version 1.1.2. In the meantime, if you want to create a
cluster configured to use a Docker registry, pass in the
       <code dir="ltr" translate="no">
        --skip-validation-docker
       </code>
       flag to
       <code dir="ltr" translate="no">
        gkectl create cluster
       </code>
       .
      </p>
     </p>
    </div>
    <h2 id="november_5_2019">
     November 5, 2019
    </h2>
    <div class="release-issue" id="vsan-datadisk-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      GKE on-prem's configuration file has a field,
      <code dir="ltr" translate="no">
       vcenter.datadisk
      </code>
      , which looks for a path to a virtual machine disk
(VMDK) file. During installation, you choose a name for the VMDK. By default,
GKE on-prem creates a VMDK and saves it to the root of your vSphere
datastore.
     </p>
     <p>
      If you are using a vSAN datastore, you need to create a folder in the
datastore in which to save the VMDK. You provide the full path to the field—for
example,
      <code dir="ltr" translate="no">
       datadisk: anthos/gke/docs/on-prem/datadisk.vmdk
      </code>
      —and
GKE on-prem saves the VMDK in that folder.
     </p>
     <p>
      When you create the folder, vSphere assigns the folder a universally unique
identifier (UUID). Although you provide the folder path to the
GKE on-prem config, the vSphere API looks for the folder's UUID.
Currently, this mismatch can cause cluster creation and upgrades to fail.
     </p>
     <p>
      A fix is targeted for version 1.1.2. In the meantime, you need to provide the
folder's UUID instead of the folder's path. Follow the workaround instructions
currently available in the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/upgrading-clusters#admin_datadisk_folder">
       upgrading clusters
      </a>
      and installation topics.
     </p>
    </div>
    <h2 id="october_25_2019">
     October 25, 2019
    </h2>
    <p>
     GKE on-prem version 1.1.1-gke.2 is now available. To download
  version 1.1.1-gke.2's OVA,
     <code dir="ltr" translate="no">
      gkectl
     </code>
     , and upgrade bundle, see
     <a href="/anthos/gke/docs/on-prem/downloads#latest">
      Downloads
     </a>
     . Then, see
     <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
      Upgrading admin workstation
     </a>
     and
     <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
      Upgrading clusters
     </a>
     .
    </p>
    <p>
     This patch version includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      <strong>
       Action required:
      </strong>
      This version upgrades the minimum
      <code dir="ltr" translate="no">
       gcloud
      </code>
      version on the admin workstation to 256.0.0. You
      should
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/upgrading-admin-workstation">
       upgrade your admin workstation
      </a>
      . Then, you should
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/upgrading-clusters">
       upgrade your clusters
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The open source
      <a class="external" href="https://github.com/coreos/toolbox">
       CoreOS toolbox
      </a>
      is now included in all GKE on-prem cluster nodes. This suite of
  tools is useful for troubleshooting node issues. See
      <a href="/anthos/gke/docs/on-prem/archive/1.1/support/toolbox">
       Debugging node issues using toolbox
      </a>
      .
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue that prevented clusters configured with OIDC from being
    upgraded.
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed
      <a class="external" href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11253">
       CVE-2019-11253
      </a>
      described in
      <a href="/anthos/gke/docs/on-prem/archive/1.1/security-bulletins#october-16-2019">
       Security bulletins
      </a>
      .
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue that caused cluster metrics to be lost due to a lost
    connection to Google Cloud. When a GKE on-prem cluster's
    connection to Google Cloud is lost for a period of time, that
    cluster's metrics are now fully recovered.
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue that caused ingestion of admin cluster metrics to be
    slower than ingesting user cluster metrics.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      For user clusters that are using static IPs and a different network than
    their admin cluster: If you overwrite the user cluster's network
    configuration, the user control plane might not be able to start. This
    occurs because it's using the user cluster's network, but allocates an IP
    address and gateway from the admin cluster.
     </p>
     <p>
      As a workaround, you can update
    each user control plane's MachineDeployment specification to use the correct
    network. Then, delete each user control plane Machine, causing the
    MachineDeployment to create new Machines:
     </p>
     <ol>
      <li>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no"># List MachineDeployments in the admin cluster
    kubectl get machinedeployments --kubeconfig [ADMIN_CLUSTER_KUBECONFIG]</pre>
      </li>
      <li>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no"># Update a user control plane MachineDeployment from your shell
    kubectl edit machinedeployment --kubeconfig [ADMIN_CLUSTER_KUBECONFIG] [MACHINEDEPLOYMENT_NAME]</pre>
      </li>
      <li>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no"># List Machines in the admin cluster
    kubectl get machines --kubeconfig [ADMIN_CLUSTER_KUBECONFIG]</pre>
      </li>
      <li>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no"># Delete user control plane Machines in the admin cluster
    kubectl delete machines --kubeconfig [ADMIN_CLUSTER_KUBECONFIG] [MACHINE_NAME]</pre>
      </li>
     </ol>
    </div>
    <h2 id="september_26_2019">
     September 26, 2019
    </h2>
    <p>
     GKE on-prem version 1.1.0-gke.6 is now available. To download
  version 1.1.0-gke.6's
     <code dir="ltr" translate="no">
      gkectl
     </code>
     and upgrade bundle, see
     <a href="/anthos/gke/docs/on-prem/downloads#latest">
      Downloads
     </a>
     . Then, see
     <a href="/anthos/gke/docs/on-prem/how-to/upgrading">
      Upgrading clusters
     </a>
     .
    </p>
    <p>
     This minor version includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The default Kubernetes version for cluster nodes
        is now version 1.13.7-gke.20 (previously 1.12.7-gke.19).
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      <strong>
       Action required:
      </strong>
      As of version 1.1.0-gke.6,
        GKE on-prem now creates vSphere
      <a class="external" href="https://www.vmware.com/products/vsphere/drs-dpm.html">
       Distributed Resource Scheduler (DRS)
      </a>
      rules for your user cluster's nodes (vSphere VMs), causing them to be
        spread across at least three physical hosts in your datacenter.
      <p>
       <strong>
        This
        feature is enabled by default for all new and existing user clusters
        running version 1.1.0-gke.6.
       </strong>
      </p>
      <p>
       The feature requires that your vSphere environment meets the
          following conditions:
      </p>
      <ul>
       <li>
        VMware DRS must be enabled. VMware DRS requires vSphere
              Enterprise Plus license edition. To learn how to enable DRS, see
        <a class="external" href="https://kb.vmware.com/s/article/1034280">
         Enabling VMware DRS in a cluster
        </a>
        .
       </li>
       <li>
        The vSphere user account provided in your GKE on-prem configuration file's
        <code dir="ltr" translate="no">
         vcenter
        </code>
        field must have the
        <code dir="ltr" translate="no">
         Host.Inventory.EditCluster
        </code>
        permission.
       </li>
       <li>
        There are at least three physical hosts available.
       </li>
      </ul>
      <p>
       If you
       <i>
        do not
       </i>
       want to enable this feature for your existing user
      clusters—for example, if you don't have enough hosts to accommdate the
      feature—perform the following steps
       <i>
        before
       </i>
       you upgrade your user
      clusters:
      </p>
      <ol>
       <li>
        Open your existing GKE on-prem configuration file.
       </li>
       <li>
        Under the
        <code dir="ltr" translate="no">
         usercluster
        </code>
        specification, add the
        <code dir="ltr" translate="no">
         antiaffinitygroups
        </code>
        field as described in the
        <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#antiaffinitygroups">
         <code dir="ltr" translate="no">
          antiaffinitygroups
         </code>
         documentation
        </a>
        :
        <pre dir="ltr" translate="no">
usercluster:
      ...
      antiaffinitygroups:
        enabled: false
</pre>
       </li>
       <li>
        Save the file.
       </li>
       <li>
        Use the configuration file to upgrade. Your clusters are upgraded, but the
            feature is not enabled.
       </li>
      </ol>
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now set the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/default-storage-class">
       default storage class
      </a>
      for your clusters.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now use
      <a class="external" href="https://github.com/container-storage-interface/spec">
       Container Storage Interface (CSI) 1.0
      </a>
      as a storage class for your clusters.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/deleting-a-user-cluster#delete_unhealthy_cluster">
       delete broken or unhealthy user clusters
      </a>
      with
      <code dir="ltr" translate="no">
       gkectl delete cluster --force
      </code>
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now
      <a href="/anthos/gke/docs/on-prem/archive/1.1/support/debug-toolbox">
       diagnose node issues
      </a>
      using the
      <code dir="ltr" translate="no">
       debug-toolbox
      </code>
      container image.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#skip_validate">
       skip validatations
      </a>
      run by
      <code dir="ltr" translate="no">
       gkectl
      </code>
      commands.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The tarball that
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      creates now includes a log of the command's output by default.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Adds
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      flag
      <code dir="ltr" translate="no">
       --seed-config
      </code>
      . When you pass the flag, it includes your
        clusters' GKE on-prem configuration file in the tarball procduced by
      <code dir="ltr" translate="no">
       snapshot
      </code>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      The
      <code dir="ltr" translate="no">
       gkeplatformversion
      </code>
      field has
        been removed from the GKE on-prem configuration file. To specify
        a cluster's version, provide the version's bundle to the
      <code dir="ltr" translate="no">
       bundlepath
      </code>
      field.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You need to add the vSphere permission,
      <code dir="ltr" translate="no">
       Host.Inventory.EditCluster
      </code>
      ,
    before you can use
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#antiaffinitygroups">
       <code dir="ltr" translate="no">
        antiaffinitygroups
       </code>
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You now specify a configuration file in
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      by passing the
      <code dir="ltr" translate="no">
       --snapshot-config
      </code>
      (previously
      <code dir="ltr" translate="no">
       --config
      </code>
      ). See
      <a href="/anthos/gke/docs/on-prem/archive/1.1/support/diagnose#diagnose_snapshot">
       Diagnosing cluster issues
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You now capture your cluster's configuration file
    with
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      by passing
      <code dir="ltr" translate="no">
       --snapshot-config
      </code>
      (previously
      <code dir="ltr" translate="no">
       --config
      </code>
      ).
        See
      <a href="/anthos/gke/docs/on-prem/archive/1.1/support/diagnose#diagnose_snapshot">
       Diagnosing cluster issues
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose
      </code>
      commands now return
        an error if you provide a user cluster's kubeconfig, rather than an admin
        cluster's kubeconfig.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Cloud Console now notifies you when an upgrade is available for a
    registered user cluster.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      A known issue prevents version 1.0.11, 1.0.1-gke.5, and 1.0.2-gke.3
      clusters using OIDC from being upgraded to version 1.1. A fix is targeted
      for version 1.1.1. If you configured a version 1.0.11, 1.0.1-gke.5, or
      1.0.2-gke.3 cluster with OIDC, you are not able to upgrade it. Create a
      version 1.1 cluster by following
      <a href="/anthos/gke/docs/on-prem/how-to/install">
       Installing GKE on-prem
      </a>
      .
     </p>
    </div>
    <h2 id="august_22_2019">
     August 22, 2019
    </h2>
    <p>
     GKE on-prem version 1.0.2-gke.3 is now available. This patch
    release includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Seesaw is now supported for manual load balancing.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now specify a different vSphere network for
        admin and user clusters.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now delete user clusters using
      <code dir="ltr" translate="no">
       gkectl
      </code>
      .
        See
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/deleting-a-user-cluster">
       Deleting a user cluster
      </a>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <code dir="ltr" translate="no">
      gkectl diagnose snapshot
     </code>
     now gets logs
    from the user cluster control planes.
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE on-prem OIDC specification has
        been updated with several new fields:
      <code dir="ltr" translate="no">
       kubectlredirecturl
      </code>
      ,
      <code dir="ltr" translate="no">
       scopes
      </code>
      ,
      <code dir="ltr" translate="no">
       extraparams
      </code>
      , and
      <code dir="ltr" translate="no">
       usehttpproxy
      </code>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Calico updated to version 3.7.4.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Cloud Monitoring's system metrics prefixed changed
        from
      <code dir="ltr" translate="no">
       external.googleapis.com/prometheus/
      </code>
      to
      <code dir="ltr" translate="no">
       kubernetes.io/anthos/
      </code>
      .
        If you are tracking metrics or alerts, update your dashbaords with the next
        prefix.
     </p>
    </div>
    <h3>
     Fixed
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      <a href="/anthos/gke/docs/on-prem/archive/1.1/security-bulletins#august-22-2019">
       Fixed a vulnerability from CVE-2019-11247
      </a>
      .
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      <a href="/anthos/gke/docs/on-prem/archive/1.1/security-bulletins#august-23-2019">
       Fixed a vulnerability in RBAC proxy
      </a>
      .
     </p>
    </div>
    <h2 id="july_30_2019">
     July 30, 2019
    </h2>
    <p>
     GKE on-prem version 1.0.1-gke.5 is now available. This patch
    release includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Published
      <a href="/anthos/gke/docs/on-prem/archive/1.1/reference/cheatsheet">
       GKE on-prem cheatsheet
      </a>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl check-config
      </code>
      now also checks
        node IP availability if you are using static IPs.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      now checks if a
        VM exists and is marked as a template in vSphere before attempting to upload
        the VM's OVA image.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Adds support for specifying a vCenter cluster,
        and resource pool in that cluster.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Upgrades F5 BIG-IP controller to version 1.9.0.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Upgrades Istio ingress controller to version 1.2.2.
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixes registry data persistence issues with the
        admin workstation's Docker registry.
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixes validation that checks whether a user
        cluster's name is already in use.
     </p>
    </div>
    <h2 id="july_25_2019">
     July 25, 2019
    </h2>
    <p>
     GKE on-prem version 1.0.11 is now available.
    </p>
    <h2 id="june_17_2019">
     June 17, 2019
    </h2>
    <p>
     GKE on-prem is now generally available. Version 1.0.10 includes the
    following changes:
    </p>
    <h3 id="beta-upgrade">
     Upgrading from beta-1.4 to 1.0.10
    </h3>
    <p>
     Before upgrading your beta clusters to the first general availability
    version, perform the steps described in
     <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/upgrading/from-beta">
      Upgrading from GKE on-prem beta to general availability
     </a>
     ,
    and review the following points:
     <ul>
      <li>
       <p>
        If you are running a beta version before beta-1.4, be sure to upgrade
                to beta-1.4 first.
       </p>
      </li>
      <li>
       <p>
        If your beta clusters are running their own L4 load balancers (not
                the default, F5 BIG-IP), you need to delete and recreate your clusters to
                run the latest GKE on-prem version.
       </p>
      </li>
      <li>
       <p>
        If your clusters were upgraded to beta-1.4 from beta-1.3, run the
                following command
        <i>
         for each user cluster
        </i>
        before upgrading:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete crd networkpolicies.crd.projectcalico.org</pre>
      </li>
      <li>
       <p>
        vCenter certificate verification is now required. (
        <code dir="ltr" translate="no">
         vsphereinsecure
        </code>
        is no longer supported.) If you're upgrading your beta 1.4 clusters to 1.0.10,
                you need to provide a vCenter trusted root CA public certificate in the
                upgrade configuration file.
       </p>
      </li>
      <li>
       <p>
        You need to upgrade
        <i>
         all
        </i>
        of your running clusters. For this
                upgrade to succeed, your clusters can't run in a mixed version state.
       </p>
      </li>
      <li>
       <p>
        You
                need to upgrade your admin clusters to the latest version first, then
                upgrade your user clusters.
       </p>
      </li>
     </ul>
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now enable the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/manual-lb">
       Manual load balancing mode
      </a>
      to configure a L4 load balancer. You can still choose to use the default load
        balancer, F5 BIG-IP.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      GKE on-prem's configuration-driven installation process has
        been updated. You now declaratively install using a singular
      <a href="/anthos/gke/docs/on-prem/archive/1.1/overview#config">
       configuration file
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Adds
      <code dir="ltr" translate="no">
       gkectl create-config
      </code>
      , which generates a configuration
        file for installing GKE on-prem, upgrading existing clusters, and
        for creating additional user clusters in an existing installation. This
        replaces the installation wizard and
      <code dir="ltr" translate="no">
       create-config.yaml
      </code>
      from
        previous versions. See the updated documentation for
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#generate_config">
       installing
            GKE on-prem
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Adds
      <code dir="ltr" translate="no">
       gkectl check-config
      </code>
      , which validates the
        GKE on-prem configuration file. See the updated documentation for
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#validate_config">
       installing
            GKE on-prem
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Adds an optional
      <code dir="ltr" translate="no">
       --validate-attestations
      </code>
      flag to
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      . This flag verifies that the container images
        included in your admin workstationwere built and signed by Google and are
        ready for deployment. See the updated documentation for
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/install#prepare">
       installing
            GKE on-prem
      </a>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Upgrades Kubernetes version to 1.12.7-gke.19. You can now
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/upgrading-clusters">
       upgrade your clusters
      </a>
      to this version. You can no longer create clusters that run Kubernetes
        version 1.11.2-gke.19.
     </p>
     <p>
      We recommend upgrading your admin cluster before you upgrade your user
        clusters.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Upgrades Istio ingress controller to version 1.1.7.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      vCenter certificate verification is now required.
      <code dir="ltr" translate="no">
       vsphereinsecure
      </code>
      is no longer supported). You provide the certificate in the
        GKE on-prem configration file's
      <code dir="ltr" translate="no">
       cacertpath
      </code>
      field.
     </p>
     <p>
      When a client calls the vCenter server, the vCenter server must prove its
        identity to the client by presenting a certificate. That certificate must be
        signed by a certificate authority (CA). The certificate is must not be
        self-signed.
     </p>
     <p>
      If you're upgrading your beta 1.4 clusters to 1.0.10, you
        need to provide a vCenter trusted root CA public certificate in the upgrade
        configuration file.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      <a href="https://cloud.google.com/anthos/gke/docs/on-prem/archive/1.1/upgrading-clusters">
       Upgrading clusters
      </a>
      can cause disruption or downtime for workloads that use
      <a class="external" href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work">
       PodDisruptionBudgets
      </a>
      (PDBs).
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      You might not be able to upgrade beta clusters that use the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/manual-lb">
       Manual load balancing mode
      </a>
      to GKE on-prem version 1.0.10. To upgrade and continue using your own load balancer with these clusters, you need to recreate the clusters.
     </p>
    </div>
    <h2 id="may_24_2019">
     May 24, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.4.7 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      In the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.4/how-to/administration/diagnose#capture_admin">
       <code dir="ltr" translate="no">
        gkectl diagnose snapshot
       </code>
      </a>
      command, the
      <code dir="ltr" translate="no">
       --admin-ssh-key-path
      </code>
      parameter is now optional.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      On May 8, 2019, we introduced a change to Connect for Anthos, the service
        that enables you to interact with your GKE on-prem clusters using
        Cloud Console. To use the new Connect for Anthos agent, you must
        re-register your clusters with Cloud Console, or you must upgrade
        to Anthos GKE on-prem beta-1.4.
     </p>
     <p>
      Your GKE on-prem clusters and the workloads running on them will continue to
        operate uninterrupted. However, your clusters will not be visible in
        Cloud Console until you re-register them or upgrade to beta-1.4.
     </p>
     <p>
      Before you re-register or upgrade, make sure your service account has
        the
      <code dir="ltr" translate="no">
       gkehub.connect
      </code>
      role. Also, if your service account has the old
        clusterregistry.connect role, it's a good idea to remove that role.
     </p>
     <p>
      Grant your service account the gkehub.connect role:
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud projects add-iam-policy-binding <var class="edit">[PROJECT_ID]</var> \
      --member="serviceAccount:<var class="edit">[SERVICE_ACCOUNT_NAME]</var>@<var class="edit">[PROJECT_ID]</var>.iam.gserviceaccount.com" \
      --role="roles/gkehub.connect"</pre>
     <p>
      If your service account has the old
      <code dir="ltr" translate="no">
       clusterregistry.connect
      </code>
      role, remove the old role:
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud projects remove-iam-policy-binding <var class="edit">[PROJECT_ID]</var> \
      --member="serviceAccount:<var class="edit">[SERVICE_ACCOUNT_NAME]</var>@<var class="edit">[PROJECT_ID]</var>.iam.gserviceaccount.com" \
      --role="roles/clusterregistry.connect"</pre>
     <p>
      Re-register you cluster, or upgrade to Anthos GKE on-prem beta-1.4.
     </p>
     <p>
      To
      <a href="/kubernetes-engine/connect/updating-agent">
       re-register your cluster
      </a>
      :
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud alpha container hub register-cluster <var class="edit">[CLUSTER_NAME]</var> \
      --context=<var class="edit">[USER_CLUSTER_CONTEXT]</var> \
      --service-account-key-file=<var class="edit">[LOCAL_KEY_PATH]</var> \
      --kubeconfig-file=<var class="edit">[KUBECONFIG_PATH]</var> \
      --project=<var class="edit">[PROJECT_ID]</var>
      </pre>
     <p>
      To
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.4/how-to/administration/upgrading-a-cluster">
       upgrade to Anthos GKE on-prem beta-1.4
      </a>
      :
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">gkectl upgrade --kubeconfig [ADMIN_CLUSTER_KUBECONFIG]</pre>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      There is an issue that prevents the Connect for Anthos agent from being
        updated to the new version during an upgrade. To work around this issue,
        run the following command after you upgrade a cluster:
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete pod gke-connect-agent-install -n gke-connect</pre>
    </div>
    <h2 id="may_13_2019">
     May 13, 2019
    </h2>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Clusters upgraded from version beta-1.2 to beta-1.3 might be affected by a
        known issue that damages the cluster's configuration file and prevents future
        cluster upgrades. This issue affects all future cluster upgrades.
     </p>
     <p>
      You can resolve this issue by deleting and recreating clusters upgraded
        from beta-1.2 to beta-1.3.
     </p>
     <p>
      To resolve the issue without deleting and recreating the cluster, you need
        to re-encode and apply each cluster's Secrets. Perform the following steps:
     </p>
     <ol>
      <li>
       Get the contents of the
       <code dir="ltr" translate="no">
        create-config
       </code>
       Secrets stored in the
            admin cluster. This must be done for the
       <code dir="ltr" translate="no">
        create-config
       </code>
       Secret
            in the kube-system namespace, and for the
       <code dir="ltr" translate="no">
        create-config
       </code>
       Secrets in each user cluster's namespace:
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl get secret create-config -n kube-system -o jsonpath={.data.cfg} | base64 -d &gt; kube-system_create_secret.yaml</pre>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl get secret create-config -n <var>[USER_CLUSTER_NAME]</var> -o jsonpath={.data.cfg} | base64 -d &gt; <var>[USER_CLUSTER_NAME]</var>_create_secret.yaml</pre>
       <li>
        For each user cluster, open the
        <code dir="ltr" translate="no">
         <var>
          [USER_CLUSTER_NAME]
         </var>
         _create_secret.yaml
        </code>
        file in an
            editor. If the values for
        <code dir="ltr" translate="no">
         registerserviceaccountkey
        </code>
        and
        <code dir="ltr" translate="no">
         connectserviceaccountkey
        </code>
        are not
        <code dir="ltr" translate="no">
         REDACTED
        </code>
        , no
            further action is required: the Secrets do not need to be re-encoded and
            written to the cluster.
       </li>
       <li>
        Open the original
        <code dir="ltr" translate="no">
         create_config.yaml
        </code>
        file in another editor.
       </li>
       <li>
        In
        <code dir="ltr" translate="no">
         <var>
          [USER_CLUSTER_NAME]
         </var>
         _create_secret.yaml
        </code>
        ,
            replace the
        <code dir="ltr" translate="no">
         registerserviceaccountkey
        </code>
        and
        <code dir="ltr" translate="no">
         connectserviceaccountkey
        </code>
        values with the values from the
            original
        <code dir="ltr" translate="no">
         create_config.yaml
        </code>
        file. Save the changed file.
       </li>
       <li>
        Repeat steps 3-5 for each
        <code dir="ltr" translate="no">
         <var>
          [USER_CLUSTER_NAME]
         </var>
         _create_secret.yaml
        </code>
        ,
            and for the
        <code dir="ltr" translate="no">
         kube-system_create_secret.yaml
        </code>
        file.
       </li>
       <li>
        Base64-encode each
        <code dir="ltr" translate="no">
         <var>
          [USER_CLUSTER_NAME]
         </var>
         _create_secret.yaml
        </code>
        file and
            the
        <code dir="ltr" translate="no">
         kube-system_create_secret.yaml
        </code>
        file:
        <pre class="devsite-click-to-copy" dir="ltr" translate="no">cat <var>[USER_CLUSTER_NAME]</var>_create_secret.yaml | base64 &gt; <var>[USER_CLUSTER_NAME]</var>_create_secret_create_secret.b64</pre>
        <pre class="devsite-click-to-copy" dir="ltr" translate="no">cat kube-system-cluster_create_secret.yaml | base64 &gt;kube-system-cluster_create_secret.b64</pre>
       </li>
       <li>
        Replace the
        <code dir="ltr" translate="no">
         data[cfg]
        </code>
        field in each Secret in the cluster
            with the contents of the corresponding file:
        <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl edit secret create-config -n <var class="edit">[USER_CLUSTER_NAME]</var>

  # kubectl edit opens the file in the shell's default text editor
  # Open `first-user-cluster_create_secret.b64` in another editor, and replace
  # the `cfg` value with the copied value
  # Make sure the copied string has no newlines in it!</pre>
       </li>
       <li>
        Repeat step 8 for each
        <code dir="ltr" translate="no">
         <var>
          [USER_CLUSTER_NAME]
         </var>
         _create_secret.yaml
        </code>
        Secret, and for the
        <code dir="ltr" translate="no">
         kube-system_create_secret.yaml
        </code>
        Secret.
       </li>
       <li>
        To ensure that the update was successful, repeat step 1.
       </li>
      </li>
     </ol>
    </div>
    <h2 id="may_7_2019">
     May 7, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.4.1 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      In the
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.4/how-to/administration/diagnose#capture_admin">
       <code dir="ltr" translate="no">
        gkectl diagnose snapshot
       </code>
      </a>
      command, the
      <code dir="ltr" translate="no">
       --admin-ssh-key-path
      </code>
      parameter is now optional.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      On May 8, 2019, we introduced a change to Connect for Anthos, the service
        that enables you to interact with your GKE on-prem clusters using
        Cloud Console. To use the new Connect for Anthos agent, you must
        re-register your clusters with Cloud Console, or you must upgrade
        to Anthos GKE on-prem beta-1.4.
     </p>
     <p>
      Your GKE on-prem clusters and the workloads running on them will continue to
        operate uninterrupted. However, your clusters will not be visible in
        Cloud Console until you re-register them or upgrade to beta-1.4.
      <p>
       Before your re-register or upgrade, make sure your service account has
            the gkehub.connect role. Also, if your service account has the old
            clusterregistry.connect role, it's a good idea to remove that role.
      </p>
      <p>
       Grant your service account the gkehub.connect role:
      </p>
      <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud projects add-iam-policy-binding <var class="edit">[PROJECT_ID]</var> \
      --member="serviceAccount:<var class="edit">[SERVICE_ACCOUNT_NAME]</var>@<var class="edit">[PROJECT_ID]</var>.iam.gserviceaccount.com" \
      --role="roles/gkehub.connect"</pre>
      <p>
       If your service account has the old clusterregistry.connect role, remove
            the old role:
      </p>
      <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud projects remove-iam-policy-binding <var class="edit">[PROJECT_ID]</var> \
      --member="serviceAccount:<var class="edit">[SERVICE_ACCOUNT_NAME]</var>@<var class="edit">[PROJECT_ID]</var>.iam.gserviceaccount.com" \
      --role="roles/clusterregistry.connect"</pre>
      <p>
       Re-register you cluster, or upgrade to Anthos GKE on-prem beta-1.4.
      </p>
      <p>
       To
       <a href="/kubernetes-engine/connect/updating-agent">
        re-register your cluster
       </a>
       :
      </p>
      <pre class="devsite-click-to-copy" dir="ltr" translate="no">gcloud alpha container hub register-cluster <var class="edit">[CLUSTER_NAME]</var> \
      --context=<var class="edit">[USER_CLUSTER_CONTEXT]</var> \
      --service-account-key-file=<var class="edit">[LOCAL_KEY_PATH]</var> \
      --kubeconfig-file=<var class="edit">[KUBECONFIG_PATH]</var> \
      --project=<var class="edit">[PROJECT_ID]</var>
      </pre>
      <p>
       To
       <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.4/how-to/administration/upgrading-a-cluster">
        upgrade to Anthos GKE on-prem beta-1.4
       </a>
       :
      </p>
      <pre class="devsite-click-to-copy" dir="ltr" translate="no">gkectl upgrade --kubeconfig [ADMIN_CLUSTER_KUBECONFIG]</pre>
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      There is an issue that prevents the Connect for Anthos agent from being
        updated to the new version during an upgrade. To work around this issue,
        run the following command after you upgrade a cluster:
     </p>
     <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete pod gke-connect-agent-install -n gke-connect</pre>
    </div>
    <h2 id="april_25_2019">
     April 25, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.3.1 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      command now has a
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.3/how-to/administration/diagnose#performing_a_dry_run_for_a_snapshot">
       <code dir="ltr" translate="no">
        --dry-run
       </code>
      </a>
      flag.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      command now supports four
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.3/how-to/administration/diagnose#snapshot_scenarios">
       scenarios
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      The
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      command now supports regular
        expressions for specifying namespaces.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Istio 1.1 is now the default
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.3/how-to/administration/upgrading-a-cluster#upgrading_the_ingress_controller">
       ingress controller
      </a>
      .
        The ingress controller runs in the
      <code dir="ltr" translate="no">
       gke-system
      </code>
      namespace for
        both admin and user clusters. This enables easier TLS management for
        Ingress. To enable ingress, or to re-enable ingress after an upgrade,
        follow the instructions under
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.3/how-to/installation/install#enabling_ingress">
       Enabling ingress
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      The
      <code dir="ltr" translate="no">
       gkectl
      </code>
      tool no longer uses Minikube and KVM for bootstrapping. This
        means you do not have to enable nested virtualization on your admin
        workstation VM.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      GKE on-prem's ingress controller uses Istio 1.1 with automatic
        Secret discovery. However, the node agent for Secret discovery may fail to
        get Secret updates after Secret deletion. So avoid deleting Secrets. If you
        must delete a Secret and Ingress TLS fails afterwards, manually restart
        the Ingress Pod in the gke-system namespace.
     </p>
    </div>
    <h2 id="april_11_2019">
     April 11, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.2.1 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      GKE on-prem clusters now automatically connect back to Google
        using
      <a href="/kubernetes-engine/connect">
       Connect for Anthos
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now run up to three control planes per user cluster.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl
      </code>
      now validates vSphere and F5 BIG-IP credentials
        creating clusters.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      A regression causes
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      commands to use
        the wrong SSH key, which prevents the command from collecting information from
        user clusters. As a workaround for support cases, you might need to SSH into
        individual user cluster nodes and manually gather data.
     </p>
    </div>
    <h2 id="april_2_2019">
     April 2, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.1.1 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You now install GKE on-prem with an
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.1/how-to/installation/getting-started#download_ova">
       Open Virtual Appliance (OVA)
      </a>
      ,
        a pre-configured virtual machine image that includes several command-line
        interface tools. This change makes
        installations easier and removes a layer of virtualization. You no longer
        need to run
      <code dir="ltr" translate="no">
       gkectl
      </code>
      inside a Docker container.
     </p>
     <p>
      If you installed GKE on-prem versions before beta-1.1.1, you
        should create a new admin workstation following the documented instructions.
        After you install the new admin workstation, copy over any SSH keys,
        configuration files, kubeconfigs, and any other files you need, from your
        previous workstation to the new one.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Added documentation for
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.1/how-to/administration/backing-up">
       backing up and restoring clusters
      </a>
      .
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now configure authentication for clusters using OIDC and ADFS.
        To learn more, refer to
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.1/how-to/security/oidc-adfs">
       Authenticating with OIDC and ADFS
      </a>
      and
      <a href="/anthos/gke/docs/on-prem/archive/1.1/concepts/authentication">
       Authentication
      </a>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You now must use an admin cluster's private key to run
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Added a configuration option during installation for deploying multi-master
        user clusters.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <a href="/kubernetes-engine/connect">
       Connect for Anthos documentation
      </a>
      has been migrated.
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue where cluster networking could be interrupted when a node
        is removed unexpectedly.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      GKE on-prem's Configuration Management has been upgraded from
        version 0.11 to 0.13. Several components of the system have been renamed.
        You need to take some steps to clean up the previous versions' resources and
        install a new instance.
     </p>
     <p>
      If you have an active instance of Configuration Management:
     </p>
     <ol>
      <li>
       <p>
        Uninstall the instance:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl -n=nomos-system delete nomos --all</pre>
      </li>
      <li>
       <p>
        Make sure that the instance's namespace has no resources:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl -n nomos-system get all</pre>
      </li>
      <li>
       <p>
        Delete the namespace:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete ns nomos-system</pre>
      </li>
      <li>
       <p>
        Delete the CRD:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete crd nomos.addons.sigs.k8s.io</pre>
      </li>
      <li>
       <p>
        Delete all kube-system resources for the operator:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl -n kube-system delete all -l k8s-app=nomos-operator</pre>
      </li>
     </ol>
     <p>
      If you don't have an active instance of Configuration Management:
     </p>
     <ol>
      <li>
       <p>
        Delete the Configuration Management namespace:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete ns nomos-system</pre>
      </li>
      <li>
       <p>
        Delete the CRD:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl delete crd nomos.addons.sigs.k8s.io</pre>
      </li>
      <li>
       <p>
        Delete all kube-system resources for the operator:
       </p>
       <pre class="devsite-click-to-copy" dir="ltr" translate="no">kubectl -n kube-system delete all -l k8s-app=nomos-operator</pre>
      </li>
     </ol>
    </div>
    <h2 id="march_12_2019">
     March 12, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.0.3 is now available. This release
    includes the following changes:
    </p>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue that caused Docker certificates to be saved to the wrong location.
     </p>
    </div>
    <h2 id="march_4_2019">
     March 4, 2019
    </h2>
    <p>
     GKE on-prem beta version 1.0.2 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now run
      <code dir="ltr" translate="no">
       gkectl version
      </code>
      to check which version of
      <code dir="ltr" translate="no">
       gkectl
      </code>
      you're running.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now
      <a href="/anthos/gke/docs/on-prem/archive/1.1/beta-1.0/how-to/administration/upgrading-a-cluster">
       upgrade user clusters
      </a>
      to future beta versions.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      <a href="/anthos-config-management/docs">
       Anthos Config Management
      </a>
      version 0.11.6 is now available.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Stackdriver Logging is now enabled on each node. By default, the logging
        agent replicates logs to your GCP project for only control plane services,
        cluster API, vSphere controller, Calico, BIG-IP controller, Envoy proxy,
        Connect for Anthos, Anthos Config Management, Prometheus and Grafana services,
        Istio control plane, and Docker. Application container logs are excluded by
        default, but can be optionally enabled.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      Stackdriver Prometheus Sidecar captures metrics for the same components as the logging agent.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      <a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">
       Kubernetes Network Policies
      </a>
      are now supported.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You can now update IP blocks in the cluster specification to expand the
        IP range for a given cluster.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      If clusters you installed during alpha were disconnected from Google after
        beta, you might need to connect them again. Refer to
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/registering-a-user-cluster">
       Manually registering a user cluster.
      </a>
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/installation/getting-started">
       Getting started
      </a>
      has been updated with steps for activating your service account and
        running
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      now only collects configuration data
        and excludes logs.  This tool is used to capture details of your environment
        prior to opening a support case.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for optional SNAT pool name configuration for F5 BIG-IP at cluster-creation time.
        This can be used to configure "--vs-snat-pool-name" value on
      <a class="external" href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.8/">
       F5 BIG-IP controller
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You now need to provide a VIP for add-ons that run in the admin cluster.
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Cluster resizing operations improved to prevent unintended node deletion.
     </p>
    </div>
    <h2 id="february_7_2019">
     February 7, 2019
    </h2>
    <p>
     GKE on-prem alpha version 1.3 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      During installation, you can now provide YAML files with
      <code dir="ltr" translate="no">
       nodeip
      </code>
      blocks to configure static IPAM.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You now need to provision a 100GB disk in vSphere Datastore.
        GKE on-prem uses the disk to store some of its vital data, such
        as etcd. See
      <a href="/anthos/gke/docs/on-prem/requirements">
       System requirements
      </a>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      You can now only provide lowercase hostnames to
      <code dir="ltr" translate="no">
       nodeip
      </code>
      blocks.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE on-prem now enforces unique names for user clusters.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Metrics endpoints and APIs that use Istio endpoints are now secured using
        mTLS and role-based access control.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      External communication by Grafana is disabled.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Improvements to Prometheus and Alertmanager health-checking.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Prometheus now uses secured port for scraping metrics.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Several updates to Grafana dashboards.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      If your vCenter user account uses a format like
      <code dir="ltr" translate="no">
       DOMAIN\USER
      </code>
      , you might need to escape the backslash (
      <code dir="ltr" translate="no">
       DOMAIN\\USER
      </code>
      ). Be sure to do this when prompted to enter the user
        account during installation.
     </p>
    </div>
    <h2 id="january_23_2019">
     January 23, 2019
    </h2>
    <p>
     GKE on-prem alpha version 1.2.1 is now available. This release
    includes the following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now use
      <code dir="ltr" translate="no">
       gkectl
      </code>
      to
      <a href="/anthos/gke/docs/on-prem/archive/1.1/how-to/administration/deleting-an-admin-cluster">
       delete admin clusters
      </a>
      .
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      commands now allow you to specify
        nodes while capturing snapshots of remote command results and files.
     </p>
    </div>
    <h2 id="january_14_2019">
     January 14, 2019
    </h2>
    <p>
     GKE on-prem alpha version 1.1.2 is now available. This release includes the
    following changes:
    </p>
    <h3>
     New Features
    </h3>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now use the
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      command to pull and push
        GKE on-prem's container images, which deprecates the
      <code dir="ltr" translate="no">
       populate_registry.sh
      </code>
      script.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      now prompts you to enter information about your
        vSphere cluster and resource pool.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now use the
      <code dir="ltr" translate="no">
       gkectl create
      </code>
      command to create and add
        user clusters to existing admin control planes by passing in an existing
        kubeconfig file when prompted during cluster creation.
     </p>
    </div>
    <div class="release-feature">
     <strong>
      FEATURE:
     </strong>
     <p>
      You can now pass in a Ingress TLS Secret for admin and user clusters
        at cluster creation time. You will see the following new prompt:
     </p>
     <p>
      <code dir="ltr" translate="no">
       Do you want to use TLS for Admin Control Plane/User Cluster ingress?
      </code>
     </p>
     <p>
      Providing the TLS Secret and certs allows
      <code dir="ltr" translate="no">
       gkectl
      </code>
      to set up
        the Ingress TLS. HTTP is not automatically disabled with TLS
        installation.
     </p>
    </div>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE on-prem now runs Kubernetes version
      <strong>
       1.11.2-gke.19
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      The default footprint for GKE on-prem has changed:
     </p>
     <ul>
      <li>
       <p>
        Minimum memory requirement for user cluster nodes is now 8192M.
       </p>
      </li>
     </ul>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE on-prem now runs minikube version
      <strong>
       0.28.0
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE Policy Management has been upgraded to version
      <strong>
       0.11.1
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl
      </code>
      no longer prompts you to provide a proxy configuration
        by default.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      There are three new ConfigMap resources in the user cluster namespace:
      <code dir="ltr" translate="no">
       cluster-api-etcd-metrics-config
      </code>
      ,
      <code dir="ltr" translate="no">
       kube-etcd-metrics-config
      </code>
      ,
        and
      <code dir="ltr" translate="no">
       kube-apiserver-config
      </code>
      . GKE on-prem uses these files to
        quickly bootstrap the metrics proxy container.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      kube-apiserver events now live in their own etcd. You can see
        kube-etcd-events in your user cluster's namespace.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Cluster API controllers now use leader election.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      vSphere credentials are now pulled from credential files.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose
      </code>
      commands now work with both admin and user
        clusters.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      can now take snapshots of remote
        files on the node, results of remote commands on the nodes, and Prometheus
        queries.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      can now take snapshots in multiple
        parallel threads.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      <code dir="ltr" translate="no">
       gkectl diagnose snapshot
      </code>
      now allows you to specify words to
        be excluded from the snapshot results.
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed issues with minikube caching that caused
        unexpected network calls.
     </p>
    </div>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      Fixed an issue with pulling F5 BIG-IP
        credentials. Credentials are now read from a credentials file instead of using environment variables.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      You might encounter the following
      <a class="external" href="https://github.com/vmware/govmomi">
       <code dir="ltr" translate="no">
        govmomi
       </code>
      </a>
      warning when you run
      <code dir="ltr" translate="no">
       gkectl prepare
      </code>
      :
      <p>
       <code dir="ltr" translate="no">
        Warning: Line 102: Unable to parse 'enableMPTSupport' for attribute 'key' on element 'Config'
       </code>
      </p>
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Resizing user clusters can cause inadvertent
        node deletion or recreation.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      PersistentVolumes can fail to mount, producing the
        error
      <code dir="ltr" translate="no">
       devicePath is empty
      </code>
      . As a workaround, delete and re-create
        the associated PersistentVolumeClaim.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Resizing IPAM address blocks if using static IP
        allocation for nodes, is not supported in alpha. To work around this, consider
        allocating more IP addresses than you currently need.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      On slow disks, VM creation can timeout and cause deployments to fail. If
        this occurs, delete all resources and try again.
     </p>
    </div>
    <h2 id="december_19_2018">
     December 19, 2018
    </h2>
    <p>
     GKE on-prem alpha 1.0.4 is now available. This release includes the
    following changes:
    </p>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      The vulnerability caused by
      <a class="external" href="https://github.com/kubernetes/kubernetes/issues/71411">
       CVE-2018-1002105
      </a>
      has been patched.
     </p>
    </div>
    <h2 id="november_30_2018">
     November 30, 2018
    </h2>
    <p>
     GKE on-prem alpha 1.0 is now available. The following changes are included in
    this release:
    </p>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      GKE on-prem alpha 1.0 runs Kubernetes 1.11.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      The default footprint for GKE on-prem has changed:
     </p>
     <ul>
      <li>
       The admin control plane runs three nodes, which use 4 CPUs and 16GB memory.
      </li>
      <li>
       The user control plane runs one node that uses 4 CPUs 16GB memory.
      </li>
      <li>
       User clusters run a minimum of three nodes, which use 4 CPUs and 16GB memory.
      </li>
     </ul>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for high-availability Prometheus setup.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for custom Alert Manager configuration.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Prometheus upgraded from
      <strong>
       2.3.2
      </strong>
      to
      <strong>
       2.4.3
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Grafana upgraded from
      <strong>
       5.0.4
      </strong>
      to
      <strong>
       5.3.4
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      kube-state-metrics upgraded from
      <strong>
       1.3.1
      </strong>
      to
      <strong>
       1.4.0
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Alert Manager upgraded from
      <strong>
       1.14.0
      </strong>
      to
      <strong>
       1.15.2
      </strong>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      node_exporter upgraded from
      <strong>
       1.15.2
      </strong>
      to
      <strong>
       1.16.0
      </strong>
      .
     </p>
    </div>
    <h3>
     Fixes
    </h3>
    <div class="release-fixed">
     <strong>
      FIXED:
     </strong>
     <p>
      The vulnerability caused by
      <a class="external" href="https://github.com/kubernetes/minikube/issues/3208">
       CVE-2018-1002103
      </a>
      has been patched.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      PersistentVolumes can fail to mount, producing the
        error
      <code dir="ltr" translate="no">
       devicePath is empty
      </code>
      . As a workaround, delete and re-create
        the associated PersistentVolumeClaim.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Resizing IPAM address blocks if using static IP
        allocation for nodes, is not supported in alpha. To work around this, consider
        allocating more IP addresses than you currently need.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      GKE on-prem alpha 1.0 does not yet pass all
        conformance tests.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Only one user cluster per admin cluster can be
        created. To create additional user clusters, create another admin
        cluster.
     </p>
    </div>
    <h2 id="october_31_2018">
     October 31, 2018
    </h2>
    <p>
     GKE on-prem EAP 2.1 is now available. The following changes are included in
    this release:
    </p>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      When you create admin and user clusters at the
        same time, you can now re-use the admin cluster's F5 BIG-IP credentials to
        create the user cluster. Also, the CLI now requires that BIG-IP credentials be
        provided; this requirement cannot be skipped using
      <code dir="ltr" translate="no">
       --dry-run
      </code>
      .
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      F5 BIG-IP controller upgraded to use the latest OSS version, 1.7.0.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      To improve stability for slow vSphere machines, cluster machine creation timeout is now 15 minutes (previously five minutes).
     </p>
    </div>
    <h2 id="october_17_2018">
     October 17, 2018
    </h2>
    <p>
     GKE on-prem EAP 2.0 is now available. The following changes are included in
    this release:
    </p>
    <h3>
     Changes
    </h3>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for GKE Connect.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for Monitoring.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for installation using private registries.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for front-ending the L7 load-balancer as a L4 VIP on F5 BIG-IP.
     </p>
    </div>
    <div class="release-changed">
     <strong>
      CHANGED:
     </strong>
     <p>
      Support for static IP allocation for nodes during cluster bootstrap.
     </p>
    </div>
    <h3>
     Known Issues
    </h3>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Only one user cluster per admin cluster can be created. To create
        additional user clusters, create another admin cluster.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      Cluster upgrades are not supported in EAP 2.0.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      On slow disks, VM creation can timeout and cause deployments to fail. If
        this occurs, delete all resources and try again.
     </p>
    </div>
    <div class="release-issue">
     <strong>
      ISSUE:
     </strong>
     <p>
      As part of the cluster bootstrapping process, a short-lived minikube
        instance is run. The minikube version used has security vulnerability
      <a class="external" href="https://github.com/kubernetes/minikube/issues/3208">
       CVE-2018-1002103
      </a>
      .
     </p>
    </div>
   </section>
  </div>
 </article>
</article>